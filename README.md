# Natural-Language-Statistical-Features-of-neural-language-Generated-pseudo-Text

### Refer Project_summary.pdf (https://github.com/sumedhachugh/Natural-Language-Statistical-Features-of-neural-language-Generated-pseudo-Text/blob/main/Project_Summary.pdf) for short description of what all I have done in the project


In this project I have generated text using LSTMs and Transformers and have shown that the neural language model based on LSTMs follows Zipf’s and Heap’s law and seen affect of hyperparameter Temperature on the quality of the text produced and the confirmation that the text produced using Transformers is of better quality than that produced by LSTMs.

#### Sherlock.txt: Dataset
#### LSTM_Text_Generator_colab.ipynb: Explored and cleaned dataset, tuned hyper-parammeters and generated LSTM model 
#### LSTM_Text_Exploration.ipynb: Generated text using model generated in previous N.B Verified Zipf's and Heap's law on data generated
#### Transformers_Text_generation.ipynb: Generated text using transformers (Credits: https://www.youtube.com/watch?v=vSN5Tn38ZIc&list=PL0Gv4uEEcwwDW6Q5AxxlektvPjeK9g_cJ&index=1. https://github.com/raghavbali/text_generation/blob/master/notebooks/text_generation_03.ipynb)


### Repos Refrenced: 

https://github.com/raghavbali/text_generation

